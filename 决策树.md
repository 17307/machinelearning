#   决策树

##  决策树ID3算法的信息论基础

-   ## 熵度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量X的熵的表达式如下：

    <img style="margin-left:auto;margin-right:auto;display:block;"  src="gs/dicisiontree/1.png">

-  ## 决策树构建

   -   ## ID3
   -   ## C4.5

<img src="img/dicissiontree3.jpg">
<img src="img/dicissiontree4.jpg">

#   sk-learn
来自`http://sklearn.apachecn.org/cn/latest/modules/tree.html#tree-multioutput`
## 二分类问题
```python
from sklearn import tree
from sklearn.datasets import load_iris  # iris数据
import graphviz  # 用于导出决策树
```
```python
def test1():
    X = [[0, 0], [1, 1]]
    Y = [0, 1]
    clf = tree.DecisionTreeClassifier()

    clf = clf.fit(X, Y)
    # 预测数据值
    print(clf.predict([[2., 2.]]))
    # 预测数据值的概率,与类别一一对应
    print(clf.predict_proba([[2., 2.]]))
```
##  多分类与展示
```python
def test2():
    iris = load_iris()
    clf = tree.DecisionTreeClassifier()
    clf = clf.fit(iris.data, iris.target)
    dot_data = tree.export_graphviz(clf, out_file=None)  # 数字形式的树
    graph = graphviz.Source(dot_data)  # 图形化树
    graph.render("img/iris")  # 生成iris.pdf

    """
    :func: export_graphviz
    出导出还支持各种美化，包括通过他们的类着色节点（或回归值），如果需要，使用显式变量和类名。Jupyter
    notebook也可以自动找出相同的模块::
    dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP
                            feature_names=iris.feature_names,  # doctest: +SKIP
                            class_names=iris.target_names,  # doctest: +SKIP
                            filled=True, rounded=True,  # doctest: +SKIP
                            special_characters=True)  # doctest: +SKIP
    """
```
##  回归问题
sk-learn 可以用来解决决策树回归算法。  
决策树通过使用 DecisionTreeRegressor 类也可以用来解决回归问题。

##  多值输出
该模块通过在 `DecisionTreeClassifier` 和 :class:`DecisionTreeRegressor` 中实现该策略来支持多输出问题。
##  使用技巧
-   对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。
-   考虑事先进行降维 `PCA` , `ICA` ，使树更好地找到具有分辨性的特征。
-   通过 `export` 功能可以可视化您的决策树。使用 `max_depth=3` 作为初始树深度，让决策树知道如何适应数据，然后再增加树的深度。
-   填充树的样本数量会增加树的每个附加级别。使用 `max_depth` 来控制输的大小防止过拟合
-   通过使用 `min_samples_split` 和 `min_samples_leaf` 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 `min_samples_leaf=5` 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 `min_samples_leaf` 保证叶结点中最少的采样数，而 `min_samples_split` 可以创建任意小的叶子，尽管在文献中 `min_samples_split` 更常见
-   在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (`sample_weight`) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (`min_weight_fraction_leaf`) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 `min_samples_leaf` 。
-   如果样本被加权，则使用基于权重的预修剪标准 `min_weight_fraction_leaf` 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分。
-   所有的决策树内部使用 `np.float32` 数组 ，如果训练数据不是这种格式，将会复制数据集
-   如果输入的矩阵X为稀疏矩阵，建议您在调用`fit`之前将矩阵X转换为稀疏的`csc_matrix` ,在调用`predict`之前将 `csr_matrix` 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级
